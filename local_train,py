# это скрипт, для обучения, сделанный из ноутбука

import os
import gc
import joblib
import numpy as np
import pandas as pd
import lightgbm as lgb
from catboost import CatBoostRegressor, Pool
from sklearn.metrics import mean_squared_error
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import AutoModel, AutoTokenizer
import torch
from pathlib import Path
from tqdm import tqdm
import warnings

warnings.filterwarnings("ignore")

# ==========================================
# 1. КОНФИГУРАЦИЯ 
# ==========================================
class constants:
    TRAIN_FILENAME = "train.csv"
    TEST_FILENAME = "test.csv"
    USER_DATA_FILENAME = "users.csv"
    BOOK_DATA_FILENAME = "books.csv"
    BOOK_GENRES_FILENAME = "book_genres.csv"
    GENRES_FILENAME = "genres.csv"
    BOOK_DESCRIPTIONS_FILENAME = "book_descriptions.csv"
    
    TFIDF_VECTORIZER_FILENAME = "tfidf_vectorizer.pkl"
    BERT_EMBEDDINGS_FILENAME = "bert_embeddings.pkl"
    BERT_MODEL_NAME = "DeepPavlov/rubert-base-cased"
    
    COL_USER_ID = "user_id"
    COL_BOOK_ID = "book_id"
    COL_TARGET = "rating"
    COL_SOURCE = "source"
    COL_PREDICTION = "rating_predict"
    COL_HAS_READ = "has_read"
    COL_TIMESTAMP = "timestamp"
    
    # Feature columns
    F_USER_MEAN_RATING = "user_mean_rating"
    F_USER_RATINGS_COUNT = "user_ratings_count"
    F_BOOK_MEAN_RATING = "book_mean_rating"
    F_BOOK_RATINGS_COUNT = "book_ratings_count"
    F_AUTHOR_MEAN_RATING = "author_mean_rating"
    F_BOOK_GENRES_COUNT = "book_genres_count"
    
    COL_GENDER = "gender"
    COL_AGE = "age"
    COL_AUTHOR_ID = "author_id"
    COL_PUBLICATION_YEAR = "publication_year"
    COL_LANGUAGE = "language"
    COL_PUBLISHER = "publisher"
    COL_AVG_RATING = "avg_rating"
    COL_GENRE_ID = "genre_id"
    COL_DESCRIPTION = "description"
    
    VAL_SOURCE_TRAIN = "train"
    VAL_SOURCE_TEST = "test"
    MISSING_CAT_VALUE = "-1"
    MISSING_NUM_VALUE = -1

class config:
    ROOT_DIR = Path(".")
    DATA_DIR = ROOT_DIR / "data"
    RAW_DATA_DIR = DATA_DIR / "raw"
    OUTPUT_DIR = ROOT_DIR / "output"
    MODEL_DIR = OUTPUT_DIR / "models"
    SUBMISSION_DIR = OUTPUT_DIR / "submissions"
    
    RANDOM_STATE = 42
    TARGET = constants.COL_TARGET
    TEMPORAL_SPLIT_RATIO = 0.8
    
    TFIDF_MAX_FEATURES = 500
    TFIDF_NGRAM_RANGE = (1, 2)
    
    BERT_BATCH_SIZE = 32
    BERT_DEVICE = "cpu" # Безопасно для локалки
    
    CAT_FEATURES = [
        constants.COL_USER_ID, constants.COL_BOOK_ID, constants.COL_GENDER,
        constants.COL_AGE, constants.COL_AUTHOR_ID, constants.COL_PUBLICATION_YEAR,
        constants.COL_LANGUAGE, constants.COL_PUBLISHER,
    ]
    
    LGB_PARAMS = {
        "objective": "rmse", "metric": "rmse", "n_estimators": 5000,
        "learning_rate": 0.01, "feature_fraction": 0.8, "bagging_fraction": 0.8,
        "bagging_freq": 1, "lambda_l1": 0.1, "lambda_l2": 0.1, "num_leaves": 31,
        "verbose": -1, "n_jobs": -1, "seed": RANDOM_STATE
    }
    
    CB_PARAMS = {
        "loss_function": "RMSE", "iterations": 5000, "learning_rate": 0.03,
        "depth": 6, "l2_leaf_reg": 3, "task_type": "CPU", "verbose": 500,
        "allow_writing_files": False
    }

for p in [config.MODEL_DIR, config.SUBMISSION_DIR]:
    p.mkdir(parents=True, exist_ok=True)

def seed_everything(seed=42):
    np.random.seed(seed)
    torch.manual_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)

# ==========================================
# 2. FEATURE ENGINEERING 
# ==========================================

def add_aggregate_features(df, train_df):
    print("Adding aggregate features...")
    # Статистики считаются ТОЛЬКО по переданному train_df
    user_agg = train_df.groupby(constants.COL_USER_ID)[config.TARGET].agg(["mean", "count"]).reset_index()
    user_agg.columns = [constants.COL_USER_ID, constants.F_USER_MEAN_RATING, constants.F_USER_RATINGS_COUNT]
    
    book_agg = train_df.groupby(constants.COL_BOOK_ID)[config.TARGET].agg(["mean", "count"]).reset_index()
    book_agg.columns = [constants.COL_BOOK_ID, constants.F_BOOK_MEAN_RATING, constants.F_BOOK_RATINGS_COUNT]
    
    author_agg = train_df.groupby(constants.COL_AUTHOR_ID)[config.TARGET].agg(["mean"]).reset_index()
    author_agg.columns = [constants.COL_AUTHOR_ID, constants.F_AUTHOR_MEAN_RATING]
    
    df = df.merge(user_agg, on=constants.COL_USER_ID, how="left")
    df = df.merge(book_agg, on=constants.COL_BOOK_ID, how="left")
    df = df.merge(author_agg, on=constants.COL_AUTHOR_ID, how="left")
    return df

def add_genre_features(df, book_genres_df):
    print("Adding genre features...")
    genre_counts = book_genres_df.groupby(constants.COL_BOOK_ID)[constants.COL_GENRE_ID].count().reset_index()
    genre_counts.columns = [constants.COL_BOOK_ID, constants.F_BOOK_GENRES_COUNT]
    return df.merge(genre_counts, on=constants.COL_BOOK_ID, how="left")

def add_text_features(df, train_df, descriptions_df):
    print("Adding TF-IDF features...")
    vectorizer_path = config.MODEL_DIR / constants.TFIDF_VECTORIZER_FILENAME
    
    train_books = train_df[constants.COL_BOOK_ID].unique()
    train_desc = descriptions_df[descriptions_df[constants.COL_BOOK_ID].isin(train_books)].copy()
    train_desc[constants.COL_DESCRIPTION] = train_desc[constants.COL_DESCRIPTION].fillna("")
    
    if vectorizer_path.exists():
        vectorizer = joblib.load(vectorizer_path)
    else:
        vectorizer = TfidfVectorizer(
            max_features=config.TFIDF_MAX_FEATURES, ngram_range=config.TFIDF_NGRAM_RANGE
        )
        vectorizer.fit(train_desc[constants.COL_DESCRIPTION])
        joblib.dump(vectorizer, vectorizer_path)
        
    all_desc = descriptions_df.set_index(constants.COL_BOOK_ID)[constants.COL_DESCRIPTION].to_dict()
    df_desc = df[constants.COL_BOOK_ID].map(all_desc).fillna("")
    
    tfidf_matrix = vectorizer.transform(df_desc)
    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=[f"tfidf_{i}" for i in range(tfidf_matrix.shape[1])], index=df.index)
    return pd.concat([df, tfidf_df], axis=1)

def add_bert_features(df, descriptions_df):
    print("Adding BERT features...")
    embeddings_path = config.MODEL_DIR / constants.BERT_EMBEDDINGS_FILENAME
    
    if embeddings_path.exists():
        print("Loading embeddings...")
        embeddings_dict = joblib.load(embeddings_path)
    else:
        print("Generating BERT embeddings...")
        tokenizer = AutoTokenizer.from_pretrained(constants.BERT_MODEL_NAME)
        model = AutoModel.from_pretrained(constants.BERT_MODEL_NAME).to(config.BERT_DEVICE)
        model.eval()
        
        unique_desc = descriptions_df.drop_duplicates(constants.COL_BOOK_ID)
        texts = unique_desc[constants.COL_DESCRIPTION].fillna("").tolist()
        ids = unique_desc[constants.COL_BOOK_ID].values
        embeddings_dict = {}
        
        for i in tqdm(range(0, len(texts), config.BERT_BATCH_SIZE)):
            batch_text = texts[i:i+config.BERT_BATCH_SIZE]
            batch_ids = ids[i:i+config.BERT_BATCH_SIZE]
            encoded = tokenizer(batch_text, padding=True, truncation=True, max_length=512, return_tensors="pt").to(config.BERT_DEVICE)
            with torch.no_grad():
                out = model(**encoded)
                # Mean pooling
                mask = encoded['attention_mask'].unsqueeze(-1)
                embeds = (out.last_hidden_state * mask).sum(1) / mask.sum(1)
            
            for bid, emb in zip(batch_ids, embeds.cpu().numpy()):
                embeddings_dict[bid] = emb
        
        joblib.dump(embeddings_dict, embeddings_path)
    
    # Map to dataframe
    # Fast numpy mapping
    matrix = np.zeros((len(df), 768), dtype=np.float32)
    book_ids = df[constants.COL_BOOK_ID].values
    for i, bid in enumerate(book_ids):
        if bid in embeddings_dict:
            matrix[i] = embeddings_dict[bid]
            
    bert_df = pd.DataFrame(matrix, columns=[f"bert_{i}" for i in range(768)], index=df.index)
    return pd.concat([df, bert_df], axis=1)

def handle_missing(df, train_df):
    print("Handling missing values...")
    global_mean = train_df[config.TARGET].mean()
    
    # Fill Aggregates
    df[constants.F_USER_MEAN_RATING] = df[constants.F_USER_MEAN_RATING].fillna(global_mean)
    df[constants.F_BOOK_MEAN_RATING] = df[constants.F_BOOK_MEAN_RATING].fillna(global_mean)
    df[constants.F_AUTHOR_MEAN_RATING] = df[constants.F_AUTHOR_MEAN_RATING].fillna(global_mean)
    
    df = df.fillna({
        constants.F_USER_RATINGS_COUNT: 0,
        constants.F_BOOK_RATINGS_COUNT: 0,
        constants.COL_AVG_RATING: global_mean,
        constants.F_BOOK_GENRES_COUNT: 0,
        constants.COL_AGE: df[constants.COL_AGE].median()
    })
    
    # Fill Categories
    for col in config.CAT_FEATURES:
        if col in df.columns:
            # Превращаем в int (код), -1 для пропусков
            df[col] = df[col].fillna(-1).astype(int).astype('category')
            
    return df

# ==========================================
# MAIN LOGIC WITH REFIT
# ==========================================
def main():
    seed_everything()
    print(">>> Loading Data...")
    
    # Грузим без parse_dates
    raw_train = pd.read_csv(config.RAW_DATA_DIR / constants.TRAIN_FILENAME)
    raw_test = pd.read_csv(config.RAW_DATA_DIR / constants.TEST_FILENAME)
    
    # FIX TIMESTAMPS
    raw_train[constants.COL_TIMESTAMP] = pd.to_datetime(raw_train[constants.COL_TIMESTAMP], errors='coerce')
    if constants.COL_TIMESTAMP in raw_test.columns:
        raw_test[constants.COL_TIMESTAMP] = pd.to_datetime(raw_test[constants.COL_TIMESTAMP], errors='coerce')
    else:
        raw_test[constants.COL_TIMESTAMP] = pd.NaT

    users = pd.read_csv(config.RAW_DATA_DIR / constants.USER_DATA_FILENAME)
    books = pd.read_csv(config.RAW_DATA_DIR / constants.BOOK_DATA_FILENAME).drop_duplicates(constants.COL_BOOK_ID)
    book_genres = pd.read_csv(config.RAW_DATA_DIR / constants.BOOK_GENRES_FILENAME)
    descriptions = pd.read_csv(config.RAW_DATA_DIR / constants.BOOK_DESCRIPTIONS_FILENAME)
    
    # Merge Metadata
    train_target = raw_train[raw_train[constants.COL_HAS_READ] == 1].copy()
    test_target = raw_test.copy()
    train_target['source'] = 'train'
    test_target['source'] = 'test'
    
    full = pd.concat([train_target, test_target], ignore_index=True)
    full = full.merge(users, on=constants.COL_USER_ID, how='left')
    full = full.merge(books, on=constants.COL_BOOK_ID, how='left')
    
    # Sort for split
    full = full.sort_values(constants.COL_TIMESTAMP)
    
    # ==========================
    # PHASE 1: VALIDATION (FIND BEST ITERATION)
    # ==========================
    print("\n=== PHASE 1: Validation (80/20) ===")
    
    # Split "Train" part into Train/Val
    train_full_mask = full['source'] == 'train'
    X_full_train = full[train_full_mask].copy()
    
    split_idx = int(len(X_full_train) * config.TEMPORAL_SPLIT_RATIO)
    
    # ВАЖНО: Разделяем ДО генерации признаков, чтобы не было утечки
    X_train_part = X_full_train.iloc[:split_idx].copy()
    X_val_part = X_full_train.iloc[split_idx:].copy()
    

    print("Generating features for Validation...")
    
    # 1. Aggregates
    X_train_feat = add_aggregate_features(X_train_part, X_train_part)
    X_val_feat = add_aggregate_features(X_val_part, X_train_part) # Val stats from Train!
    
    # 2. Genre
    X_train_feat = add_genre_features(X_train_feat, book_genres)
    X_val_feat = add_genre_features(X_val_feat, book_genres)
    
    # 3. Text (TF-IDF + BERT)
    X_train_feat = add_text_features(X_train_feat, X_train_part, descriptions)
    X_val_feat = add_text_features(X_val_feat, X_train_part, descriptions) # Fit on Train
    
    X_train_feat = add_bert_features(X_train_feat, descriptions)
    X_val_feat = add_bert_features(X_val_feat, descriptions)
    
    # 4. Handle Missing
    X_train_feat = handle_missing(X_train_feat, X_train_part)
    X_val_feat = handle_missing(X_val_feat, X_train_part)
    
    # Prepare for Training
    drop_cols = [constants.COL_SOURCE, config.TARGET, constants.COL_PREDICTION, constants.COL_TIMESTAMP, 'title', 'author_name', 'description', 'has_read']
    
    y_train = X_train_feat[config.TARGET]
    y_val = X_val_feat[config.TARGET]
    
    X_train_final = X_train_feat.drop(columns=[c for c in drop_cols if c in X_train_feat.columns])
    X_val_final = X_val_feat.drop(columns=[c for c in drop_cols if c in X_val_feat.columns])
    
    cat_feats = [c for c in X_train_final.columns if X_train_final[c].dtype.name == 'category']
    
    # LGBM Training (Find Best Iteration)
    print("\nTraining LightGBM (Val)...")
    lgb_tr = lgb.Dataset(X_train_final, y_train, categorical_feature=cat_feats)
    lgb_val = lgb.Dataset(X_val_final, y_val, categorical_feature=cat_feats, reference=lgb_tr)
    
    model_lgb = lgb.train(
        config.LGB_PARAMS, lgb_tr, valid_sets=[lgb_tr, lgb_val],
        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(100)]
    )
    best_iter_lgb = model_lgb.best_iteration
    rmse_lgb = np.sqrt(mean_squared_error(y_val, model_lgb.predict(X_val_final)))
    print(f"LGBM Val RMSE: {rmse_lgb:.4f}")
    
    # CatBoost Training
    print("\nTraining CatBoost (Val)...")
    cb_pool_tr = Pool(X_train_final, y_train, cat_features=cat_feats)
    cb_pool_val = Pool(X_val_final, y_val, cat_features=cat_feats)
    
    model_cb = CatBoostRegressor(**config.CB_PARAMS)
    model_cb.fit(cb_pool_tr, eval_set=cb_pool_val, early_stopping_rounds=100)
    best_iter_cb = model_cb.get_best_iteration()
    rmse_cb = np.sqrt(mean_squared_error(y_val, model_cb.predict(X_val_final)))
    print(f"CatBoost Val RMSE: {rmse_cb:.4f}")
    
    # Clean memory
    del X_train_feat, X_val_feat, X_train_final, X_val_final
    gc.collect()
    
    # ==========================
    # PHASE 2: REFIT ON FULL DATA
    # ==========================
    print("\n=== PHASE 2: Refit on Full Data ===")
    
    # Теперь считаем статистики по ВСЕМУ трейну (X_full_train)
    # И применяем к X_full_train и X_test
    X_test_base = full[full['source'] == 'test'].copy()
    
    print("Generating features for Full Train + Test...")
    
    # 1. Aggregates (Stats from FULL train)
    X_full_feat = add_aggregate_features(X_full_train, X_full_train)
    X_test_feat = add_aggregate_features(X_test_base, X_full_train)
    
    # 2. Genre
    X_full_feat = add_genre_features(X_full_feat, book_genres)
    X_test_feat = add_genre_features(X_test_feat, book_genres)
    
    # 3. Text
    X_full_feat = add_text_features(X_full_feat, X_full_train, descriptions)
    X_test_feat = add_text_features(X_test_feat, X_full_train, descriptions)
    
    X_full_feat = add_bert_features(X_full_feat, descriptions)
    X_test_feat = add_bert_features(X_test_feat, descriptions)
    
    # 4. Missing
    X_full_feat = handle_missing(X_full_feat, X_full_train)
    X_test_feat = handle_missing(X_test_feat, X_full_train)
    
    # Prepare
    y_full = X_full_feat[config.TARGET]
    X_full_final = X_full_feat.drop(columns=[c for c in drop_cols if c in X_full_feat.columns])
    X_test_final = X_test_feat.drop(columns=[c for c in drop_cols if c in X_test_feat.columns])
    
    # Refit LGBM
    print(f"Refitting LGBM ({best_iter_lgb} iters)...")
    p_lgb = config.LGB_PARAMS.copy()
    p_lgb['n_estimators'] = int(best_iter_lgb * 1.15) # +15% т.к. данных больше
    lgb_full_ds = lgb.Dataset(X_full_final, y_full, categorical_feature=cat_feats)
    model_lgb_final = lgb.train(p_lgb, lgb_full_ds)
    
    # Refit CatBoost
    print(f"Refitting CatBoost ({best_iter_cb} iters)...")
    p_cb = config.CB_PARAMS.copy()
    p_cb['iterations'] = int(best_iter_cb * 1.15)
    cb_full_pool = Pool(X_full_final, y_full, cat_features=cat_feats)
    model_cb_final = CatBoostRegressor(**p_cb)
    model_cb_final.fit(cb_full_pool)
    
    # Predict
    print("Generating predictions...")
    p_lgb = model_lgb_final.predict(X_test_final)
    p_cb = model_cb_final.predict(X_test_final)
    
    # Save
    sub = raw_test[[constants.COL_USER_ID, constants.COL_BOOK_ID]].copy()
    
    # LGBM - по итогу я его не взял
    # sub[constants.COL_PREDICTION] = np.clip(p_lgb, 0, 10)
    # sub.to_csv(config.SUBMISSION_DIR / "submission_lgbm_refit.csv", index=False)
    
    # CatBoost - Я выбрал его
    sub[constants.COL_PREDICTION] = np.clip(p_cb, 0, 10)
    sub.to_csv(config.SUBMISSION_DIR / "submission_catboost_refit.csv", index=False)
    
    # Ensemble - и его тоже, но чуть-чуть оказался похуже
    w_lgb = 1 / (rmse_lgb**2)
    w_cb = 1 / (rmse_cb**2)
    total = w_lgb + w_cb
    p_ens = p_lgb * (w_lgb/total) + p_cb * (w_cb/total)
    
    sub[constants.COL_PREDICTION] = np.clip(p_ens, 0, 10)
    sub.to_csv(config.SUBMISSION_DIR / "submission_ensemble_refit.csv", index=False)
    
    print("Done! Files in output/submissions/")

if __name__ == "__main__":
    main()
